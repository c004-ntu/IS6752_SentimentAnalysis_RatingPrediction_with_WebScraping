{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Scraping User Profiles </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisites:\n",
    "* Make sure to place chromedriver.exe in the same directory as your code, in \"\\chromedriver-win64\" subfolder.\n",
    "* Make sure you have mysql installed. \n",
    "* Make sure you have a database named \"scraping\". Otherwise, execute this command.\n",
    "<br>CREATE DATABASE scraping;\n",
    "* Make sure you have table named \"movie\" in scraping database. Otherwise, execute this command.\n",
    "<br>\n",
    "<span style=\"color:green;\">CREATE TABLE movie (movieid VARCHAR(255) PRIMARY KEY, movie_title VARCHAR(255), movie_year INT, movie_release_date DATE, movie_genre VARCHAR(255), movie_rating INT, movie_href VARCHAR(255), movie_desc TEXT, movie_cast TEXT, movie_tag TEXT, budget DECIMAL(15, 2), revenue DECIMAL(15, 2), page_count INT, download_flag INT);</span>\n",
    "* Make sure you have table named \"rating\" in scraping database. Otherwise, execute this command.\n",
    "<br>\n",
    "<span style=\"color:green;\">CREATE TABLE rating (reviewid VARCHAR(255) PRIMARY KEY, userid VARCHAR(255), movieid VARCHAR(255), movie_title VARCHAR(255), rating INT, review_date DATE, review_text TEXT, user_href VARCHAR(255), review_href VARCHAR(255), download_flag INT);</span>\n",
    "* Make sure you have table named \"user\" in scraping database. Otherwise, execute this command.\n",
    "<br>\n",
    "<span style=\"color:green;\">CREATE TABLE user (userid VARCHAR(50) PRIMARY KEY, user_name VARCHAR(100), user_href VARCHAR(255), user_join_date DATE);</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.options import Options \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage(driver, url):\n",
    "    driver.get(url)\n",
    "    time.sleep(5) # Wait for the page to load, also respecting TMDB rate limit: 40 request per 10 seconds\n",
    "    bsObj = BeautifulSoup(driver.page_source, 'lxml') # Get the page source and parse it with Beautiful Soup\n",
    "    return bsObj\n",
    "\n",
    "def download_Users (driver, conn, cur, main_url):\n",
    "\n",
    "    count = 0\n",
    "    unique_user_df_len = 0\n",
    "\n",
    "    userid = \"\"\n",
    "    user_href = \"\"\n",
    "    user_join_date = \"\"\n",
    "    user_genres = \"\"\n",
    "\n",
    "    filtered_rating_df = select_rating (cur)\n",
    "    filtered_rating_df_len = len(filtered_rating_df)\n",
    "\n",
    "    progress_bar = tqdm(desc='User profile download progress', total=filtered_rating_df_len)  # progress bar\n",
    "\n",
    "    for index, row in filtered_rating_df.iterrows():\n",
    "        try:\n",
    "            url = main_url + row['user_href']\n",
    "            # print(url)\n",
    "\n",
    "            reviewid = row['reviewid']\n",
    "            userid = row['userid']\n",
    "            user_href = row['user_href']\n",
    "            # print(reviewid, userid, user_href)\n",
    "\n",
    "            # Open the page\n",
    "            bsObj = getPage(driver, url)\n",
    "            \n",
    "            time.sleep(1)\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            # Find all review containers\n",
    "            h2_tag = bsObj.find('h2')\n",
    "            user_name = h2_tag.get_text().strip()\n",
    "            # print(f'{user_name}')\n",
    "\n",
    "            user_join_date = '1900-01-01'\n",
    "            if 'deleted' not in user_name and 'suspended' not in user_name and 'can\\'t find' not in user_name: # handling deleted user account\n",
    "                h3_tag = bsObj.find('h3')\n",
    "                if h3_tag:\n",
    "                    user_join_text = re.search(r'(\\w+) (\\d{4})', h3_tag.get_text())\n",
    "                    user_join_date = datetime.strptime(user_join_text[0], \"%B %Y\").date()\n",
    "                else:\n",
    "                    user_join_date = '1900-01-01'\n",
    "            # print(user_join_date)\n",
    "\n",
    "            # print(f'{userid}, {user_name}, {user_href}, {user_join_date}')\n",
    "\n",
    "            # Insert new record to database\n",
    "            insert_user_to_db(cur, userid, user_name, user_href, user_join_date)\n",
    "            \n",
    "            # Log entry to a text file in case of download stops due to reached limit\n",
    "            with open('log_user.txt', 'a') as file:\n",
    "                file.write(f'{userid}, {user_name}, {user_href}, {user_join_date}\\n')\n",
    "\n",
    "            count +=1\n",
    "            \n",
    "            # Update download_flag in rating table. This is to address the daily request limit in TMDB. Only movies without rating will be downloaded in next session.       \n",
    "            update_rating_downloadflag(cur, reviewid)\n",
    "            conn.commit()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {user_href}: {e}\")\n",
    "            print(f'{userid}, {user_name}, {user_href}, {user_join_date}')\n",
    "            continue\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    user_df = select_all_user (cur)\n",
    "    \n",
    "    return user_df\n",
    "\n",
    "def insert_user_to_db (cursor, userid, user_name, user_href, user_join_date):\n",
    "    # Insert new if user record is not found, else update the user record to keep user info in db up-to-date\n",
    "    sql = '''\n",
    "    INSERT INTO user (userid, user_name, user_href, user_join_date)\n",
    "    VALUES (%s, %s, %s, %s)\n",
    "    ON DUPLICATE KEY UPDATE\n",
    "        user_name = values(user_name),\n",
    "        user_href = values(user_href),\n",
    "        user_join_date = values(user_join_date);\n",
    "    '''\n",
    "    cursor.execute (\"USE scraping;\")\n",
    "    cursor.execute(sql, (userid, user_name, user_href, user_join_date))\n",
    "\n",
    "def select_all_rating (cursor):\n",
    "    cursor.execute (\"USE scraping;\")\n",
    "    cursor.execute(\"SELECT * from rating;\")\n",
    "    rows = cursor.fetchall()\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    df_mysql = pd.DataFrame(rows, columns=columns)\n",
    "    return df_mysql\n",
    "\n",
    "def select_all_user (cursor):\n",
    "    cursor.execute (\"USE scraping;\")\n",
    "    cursor.execute(\"SELECT * from user;\")\n",
    "    rows = cursor.fetchall()\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    df_mysql = pd.DataFrame(rows, columns=columns)\n",
    "    return df_mysql\n",
    "\n",
    "def select_rating (cursor):\n",
    "    cursor.execute (\"USE scraping;\")\n",
    "    cursor.execute(\"SELECT rating.* from rating left join movie on movie.movieid = rating.movieid where rating.download_flag = 0 or rating.download_flag is null order by movie.page_count, movie.movieid, rating.reviewid;\")\n",
    "    rows = cursor.fetchall()\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    df_mysql = pd.DataFrame(rows, columns=columns)\n",
    "    return df_mysql\n",
    "\n",
    "def update_rating_downloadflag (cursor, reviewid):\n",
    "    sql = \"UPDATE rating SET download_flag = 1 where reviewid ='\" + reviewid + \"';\"\n",
    "    cursor.execute (\"USE scraping;\")\n",
    "    cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "main_url = \"https://www.themoviedb.org\"\n",
    "user_agent = {\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\"}\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless=new\")\n",
    "# chrome_options.add_argument(\"--window-position=-2400,-2400\") # hide window\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "chrome_options.add_argument(\"--disable-infobars\")\n",
    "chrome_options.add_argument(\"--disable-web-security\")\n",
    "chrome_options.add_argument(\"--disable-site-isolation-trials\")\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "chrome_options.add_argument(\"--disable-blink-features=BlockCredentialedSubresources\")\n",
    "chrome_options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "# Set up the WebDriver\n",
    "chrome_driver = os.getcwd() + \"\\\\chromedriver-win64\" + \"\\\\chromedriver.exe\"\n",
    "service = Service(chrome_driver)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Connect to database\n",
    "try:\n",
    "    conn = pymysql.connect(host='127.0.0.1', user='root', passwd='root', db='scraping')\n",
    "    cur = conn.cursor()\n",
    "    print(\"Connection successful!\")\n",
    "except pymysql.MySQLError as e:\n",
    "    print(f\"Error connecting to the database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a09ba2475c843feb4b638106aa0dc28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User profile download progress:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1203, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_href</th>\n",
       "      <th>user_join_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>007ace</td>\n",
       "      <td>007ace</td>\n",
       "      <td>/u/007ace</td>\n",
       "      <td>2016-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15bq1a05k9</td>\n",
       "      <td>Waseem Farooq Shaik</td>\n",
       "      <td>/u/15bq1a05k9</td>\n",
       "      <td>2019-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userid            user_name      user_href user_join_date\n",
       "0      007ace               007ace      /u/007ace     2016-07-01\n",
       "1  15bq1a05k9  Waseem Farooq Shaik  /u/15bq1a05k9     2019-01-01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_df = download_Users (driver, conn, cur, main_url)\n",
    "print(user_df.shape)\n",
    "display(user_df.head(2))\n",
    "\n",
    "# Export movie to csv\n",
    "user_df.to_csv(\"user.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ChromiumDriver.quit of <selenium.webdriver.chrome.webdriver.WebDriver (session=\"dcd57b3e1d46f5bd20824d6525a84e38\")>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cur.close()\n",
    "conn.close()\n",
    "driver.close\n",
    "driver.quit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
